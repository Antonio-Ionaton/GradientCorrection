{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"200_ Synthetic_train.ipynb","provenance":[{"file_id":"1YlxyRnJsflp_1FbUmbBdTiJvmtaIQmYs","timestamp":1635121179593},{"file_id":"1USR5YrGLrs8tS2Wol7o_6kwpbwgxzMv4","timestamp":1635117531627},{"file_id":"1qsWqEJYP7gEjGvpuckOnkcLXYeKeKwoo","timestamp":1634798853451},{"file_id":"14Gm9XytIUD6c-P2K50oxSeY7d4I1309O","timestamp":1634113119976},{"file_id":"1b1nAIcDw2Et8yGqaWv1PA6Dm1XyYB6S2","timestamp":1622228724572},{"file_id":"1FwAAF5_sRRblCB_J38L-B1bN5QAcNMGF","timestamp":1616376680100},{"file_id":"1TJ0gItegNg2nTKOHxu7l3QHx8SZD5s7H","timestamp":1612672593083},{"file_id":"1f2m5D6Ha4HNLvyaNUpZV_DSWrtFNMuZ2","timestamp":1607370920851},{"file_id":"14ZWoWocKk2GUIT-w-95-rMfAfYKEi7Az","timestamp":1605153975791},{"file_id":"1m2k7qVgN3tFJ8RhJHnmNitEYSJJ5XB9J","timestamp":1604200411900},{"file_id":"1MebIz4_N34ATdl8UjTJeItO5iG3kFoyv","timestamp":1603349013260},{"file_id":"1bS_s4y_m9rLJbWySpZeBJFWuRQqyrs9O","timestamp":1602621417600},{"file_id":"1ZBeLOm9VXRe0shAieo6Wx2BM4nH3uqjc","timestamp":1593627015272},{"file_id":"1LoSL794CM4V74_i9J1ru6vfOpW5Xsf0C","timestamp":1584409111653}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"MplBnNlqZkhO"},"source":["# This notebook is a descendent from \"Gradient Correction 1.ipynb\" i  \"Gradient Correction 2.ipynb\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aMaF4EfQnTYb","executionInfo":{"status":"ok","timestamp":1635118761264,"user_tz":240,"elapsed":15660,"user":{"displayName":"Antonio May","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjpf33Upr1LSmftY4pnznC_a09tvowS-WzBKm_M=s64","userId":"15187085139794929260"}},"outputId":"a1f6a7e6-a854-4beb-d4e4-10eba5690f17"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PerEQD5vpGYo","executionInfo":{"status":"ok","timestamp":1635118781910,"user_tz":240,"elapsed":4628,"user":{"displayName":"Antonio May","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjpf33Upr1LSmftY4pnznC_a09tvowS-WzBKm_M=s64","userId":"15187085139794929260"}},"outputId":"9959b5df-8ccf-40cf-90e1-4829e013e941"},"source":["#!pip uninstall gradientcorrection  \n","!pip install https://github.com/Antonio-Ionaton/GradientCorrection/tarball/main"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting https://github.com/Antonio-Ionaton/GradientCorrection/tarball/main\n","  Downloading https://github.com/Antonio-Ionaton/GradientCorrection/tarball/main\n","\u001b[K     / 2.6 MB 7.0 MB/s\n","\u001b[?25hBuilding wheels for collected packages: gradientcorrection\n","  Building wheel for gradientcorrection (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gradientcorrection: filename=gradientcorrection-0.1-py3-none-any.whl size=8556 sha256=be239a7f0d8c5c22d3dd27a1ff6a6679d592e8d8b2402e5246b7d991c5790772\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-te2i0tm9/wheels/03/4c/0a/9516d8fa386ed9a21d1aa50e8cc5b1f6d6f576195897b24409\n","Successfully built gradientcorrection\n","Installing collected packages: gradientcorrection\n","Successfully installed gradientcorrection-0.1\n"]}]},{"cell_type":"code","metadata":{"id":"E-dhigBNpPSv"},"source":["import gradientcorrection\n","from gradientcorrection import layers, utils, metrics, helper, explain, model_zoo, geomath"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l8y9STidpwq4"},"source":["############################################################################ TRAINING\n","\n","\n","import os\n","import numpy as np\n","from six.moves import cPickle\n","from tensorflow import keras\n","\n","#------------------------------------------------------------------------\n","\n","num_trials = 50  \n","model_names = ['cnn-dist', 'cnn-local'] \n","activations = ['relu', 'exponential']  \n","\n","results_path = utils.make_directory('drive/My Drive/results', 'task3')  \n","params_path = utils.make_directory(results_path, 'model_params')  \n","\n","#------------------------------------------------------------------------\n","\n","\n","# load data\n","data_path = 'drive/My Drive/data/synthetic_code_dataset.h5'\n","data = helper.load_data(data_path)  \n","x_train, y_train, x_valid, y_valid, x_test, y_test = data\n","\n","#------------------------------------------------------------------------\n","\n","with open(os.path.join(results_path, 'task3_classification_performance.tsv'), 'w') as f:\n","    f.write('%s\\t%s\\t%s\\n'%('model', 'ave roc', 'ave pr'))\n","\n","    results = {}\n","    for model_name in model_names:\n","        for activation in activations:\n","            base_name = model_name+'_'+activation\n","            print(base_name)\n","            results[base_name] = {}\n","            \n","            trial_roc_mean = []\n","            trial_roc_std = []\n","            trial_pr_mean = []\n","            trial_pr_std = []\n","            for trial in range(num_trials):\n","                keras.backend.clear_session()\n","                \n","                # load model\n","                model = helper.load_model(model_name, activation=activation)  \n","                name = base_name+'_'+str(trial)\n","                print('model: ' + name)\n","\n","                # compile model\n","                helper.compile_model(model)  #helper.compile_model(model)\n","\n","                # setup callbacks\n","                callbacks = helper.get_callbacks(monitor='val_auroc', patience=20,   #callbacks = helper.get_callbacks(monitor='val_auroc', patience=20,\n","                                          decay_patience=5, decay_factor=0.2)   #it was 5\n","\n","                # fit model\n","                history = model.fit(x_train, y_train, \n","                                    epochs=100,\n","                                    batch_size=100, \n","                                    shuffle=True,\n","                                    validation_data=(x_valid, y_valid), \n","                                    callbacks=callbacks)\n","\n","                # save model\n","                weights_path = os.path.join(params_path, name+'.hdf5')\n","                model.save_weights(weights_path)\n","\n","                # predict test sequences and calculate performance metrics\n","                predictions = model.predict(x_test)                \n","                mean_vals, std_vals = metrics.calculate_metrics(y_test, predictions, 'binary')\n","\n","                trial_roc_mean.append(mean_vals[1])\n","                trial_roc_std.append(std_vals[1])\n","                trial_pr_mean.append(mean_vals[2])\n","                trial_pr_std.append(std_vals[2])\n","\n","\n","            results[base_name] = [np.array(trial_roc_mean), np.array(trial_pr_mean)]\n","            f.write(\"%s\\t%.3f+/-%.3f\\t%.3f+/-%.3f\\n\"%(base_name, \n","                                                      np.mean(trial_roc_mean),\n","                                                      np.std(trial_roc_mean), \n","                                                      np.mean(trial_pr_mean),\n","                                                      np.std(trial_pr_mean)))\n","\n","# save results\n","file_path = os.path.join(results_path, 'task3_performance_results.pickle')\n","with open(file_path, 'wb') as f:\n","    cPickle.dump(results, f, protocol=cPickle.HIGHEST_PROTOCOL)\n","\n"],"execution_count":null,"outputs":[]}]}